<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP&display=swap" rel="stylesheet">
  <title>White Box Testing - Mosaic Project</title>
  <style>
    body {
      font-family: 'Noto Sans JP', Arial, sans-serif;
      background-color: hsl(0, 0%, 0%);
      color: hsl(42, 100%, 80%);
      padding: 2rem;
    }
    .page {
      padding: 3rem; /* 20px */
      margin: 1.25rem auto; /* 20px */
      max-width: 46.875rem; /* 750px */
      background-color: hsl(0, 0%, 0%);
      box-shadow: 0 0 0.625rem hsl(42, 100%, 50%); /* 10px */
    }
    h1 {
      font-size: 1.5rem;
      text-align: center;
      color: hsl(42, 100%, 84%);
      text-transform: uppercase;
      margin-top: 1.5rem;
    }
    h2 {
      font-size: 1.25rem;
      color: hsl(42, 100%, 84%);
      text-align: center;
      margin-top: 2rem;
    }
    h3 {
      font-size: 1.125rem;
      color: hsl(42, 100%, 75%);
      margin-top: 1.5rem;
    }
    p, li {
      color: hsl(42, 100%, 90%);
      text-align: justify;
      margin-bottom: 1rem;
    }
    .term {
      font-family: 'Courier New', Courier, monospace;
/*      background-color: hsl(0, 0%, 19%); */
      padding: 0.125rem 0.25rem;
      border-radius: 0.125rem;
      text-decoration: underline;
/*      font-style: italic;*/
      color: hsl(42, 100%, 95%);
    }
    code {
      font-family: 'Courier New', Courier, monospace;
      background-color: hsl(0, 0%, 25%);
      padding: 0.125rem 0.25rem;
      color: hsl(42, 100%, 90%);
    }

    table {
      border-collapse: collapse;
      width: 100%;
    }

    tr {
      page-break-inside: avoid;
      page-break-after: auto;
    }

    th, td {
      padding: 0.125rem;;
/*      hsl(0, 0%, 86.7%) */
      text-align: left;
    }

  </style>
</head>
<body>
  <div class="page">
    <header>
      <h1>An Introduction to Structured Testing</h1>
      <p>© 2024 Thomas Walker Lynch - All Rights Reserved.</p>
    </header>


    <h2>Introduction</h2>

    <p>This guide provides a general overview of testing concepts. It is
      not a reference manual for the Mosaic Testbench itself. At the
      time of writing, no such reference document exists, so developers and
      testers are advised to consult the source code directly for implementation
      details. A small example can be found in the <code>Test_MockClass</code>
      file within the tester directory. Other examples can be found in projects
      that make use of Mosaic.</p>

    <p>A typical testing setup comprises three main components:
    the <span class="term">Testbench</span>, the <span class="term">test
    routines</span>, and a collection of <span class="term">units under
    test</span> (UUTs). Here, a UUT is any individual software or hardware
    component intended for testing. Because this guide focuses on software, we
    use the term <span class="term">RUT</span> (routine under test) to denote
    the unit under test in software contexts. Although we use software-centric
    terminology, the principles outlined here apply equally to hardware
    testing.</p>

    <p>Each test routine supplies inputs to a RUT, collects the resulting
      outputs, and determines whether the test passes or fails based on those
      values. A given test routine might repeat this procedure for any number
      of <span class="term">test cases</span>. The final result from the test
      routine is then relayed to the Testbench. Testers and developers write
      the test routines and place them into the Testbench.</p>

    <p>Mosaic is a Testbench. It serves as a structured environment for
    organizing and executing test routines, and it provides a library of utility
    routines for assisting the test writer. When run, the Testbench sequences
    through the set of test routines, one by one, providing each test routine
    with an interface to control and examine standard input and output. Each 
    test routine, depending on its design, might in turn sequence through
    test cases. During execution, the test
    bench records pass/fail results, lists the names of the test routines that failed,
    and generates a summary report with pass/fail totals.</p>

    <p>At the time of this writing, Mosaic does not provide features for
      breaking up large test runs into parallel pieces and then load balancing
      those pieces. Perhaps such a feature will be developed for a future version.
      However, this does not prevent an enterprising tester from running multiple
      Mosaic runs with different test routines in parallel in an ad hoc manner, or
      with other tools.</p>

    <h2>Function versus Routine</h2>

    <p>A routine is an encapsulated sequence of instructions, with a symbol
      table for local variables, and an interface for importing and exporting
      data through the encapsulation boundary. This interface
      maps <span class="term">arguments</span> from a caller
      to <span class="term">parameters</span> within the routine, enabling data
      transfer at runtime. In the context of testing, the arguments that bring
      data into the routine are referred to as
      <span class="term">inputs</span>, while those that carry data out are called
      <span class="term">outputs</span>. Notably, in programming, outputs are often called 
      <span class="term">return values</span>.</p>

   <p>In computer science, a <span class="term">pure function</span> is a routine
     in which outputs depend solely on the provided inputs, without reference to
     any internal state or memory that would persist across calls. A pure function
     produces the same output given the same inputs every time it is called. 
     Side effects, such as changes to external states or reliance on external
     resources, are not present in pure functions; any necessary interactions 
     with external data must be represented explicitly as inputs or outputs. 
     By definition, a function produces a single output, though this output can 
     be a collection, such as a vector or set.</p>

    <p>Routines with internal state variables that facilitate temporal behavior
      can produce outputs that depend on the sequence and values of prior
      inputs. This characteristic makes such routines challenging to
      test. Generally, better testing results are achieved when testing pure
      functions, where outputs depend only on current inputs.</p>


    <h2>Block and Integration</h2>

    <p>A test routine provides inputs to a RUT and collects its outputs, often
    doing so repeatedly in a sequence of test cases. The test routine then
    evaluates these values to determine if the test has passed or failed.</p>

    <p>When a test routine evaluates a RUT that corresponds to a single function
    or module within the program, it performs a <span class="term">block
    test</span>.</p>

    <p>When a test routine evaluates a RUT that encompasses multiple program
    components working together, it is conducting
    an <span class="term">integration test</span>.</p>

    <p>Integration tests typically involve combining substantial components of a
    program that were developed independently. Such tests can occur later in the
    project timeline, where they can reveal complex and unforeseen interactions
    between components when there is not adequate time to deal with them. To
    help address these challenges, some software development methodologies
    recommend to instead introducing simplified versions of large components
    early in the development process, and to then refine them over time.</p>

    <h2>Failures and Faults</h2>

    <p>A test routine has two primary responsibilities: firstly in supplying inputs
      and collecting outputs from the RUT, and secondly in determining whether the RUT
      passed or failed the test. This second responsibility is handled by
      the <span class="term">failure decider</span>. When the failure decider is not
      an explicit function in the test routine,its functionality will still be present
       in the test routines logic.</p>

    <p>A given failure decider might produce <span class="term">false
    positive</span> or <span class="term">false negative</span> results. A
    false positive occurs when the failure decider indicates that a test has
    passed when it should have failed; hence, this is also known as
    a <span class="term">false pass</span>. Conversely, a false negative occurs
    when the decider indicates failure when the test should have passed; hence, this also
    known as a <span class="term">false fail</span>. An <span class="term">ideal
    failure decider</span> would produce neither false passes nor false
    fails.</p>

    <p>In a typical testing workflow, passing tests receive no further
    scrutiny. In contrast, failed tests are further examined to locate the
    underlying fault. Thus, for such a workflow, false fails are likely to be
    caught in the debugger, while false passes might go undetected until
    release, then be discovered by users. Early in the project timeline, this
    effect can be mitigated by giving passing cases more scrutiny, essentially
    spot-checking the test environment. Later, in regression testing, the volume
    of passing cases causes spot-checking to be ineffective. Alternative
    strategies include redundant testing, better design of the failure decider,
    or employing other verification techniques.</p>

    <p>A failure occurs when there is a deviation between the <span class="term">observed output</span> from a RUT and the <span class="term">ideal output</span>. When the ideal output is unavailable, a <span class="term">reference output</span> is often used in its place. When using reference outputs, the accuracy of test results depends on both the accuracy of the failure decider and the accuracy of the reference outputs themselves.</p>

    <p>Some testers will refer to an <span class="term">observed output</span> as an <em>actual output</em>. Additionally, some testers will call <span class="term">reference outputs</span> <em>golden values</em>, particularly when those values are considered highly accurate. However, the terminology introduced earlier aligns more closely with that used in scientific experiments, which is fitting since testing is a form of experimentation.</p>

    <p>A fault is a flaw in the design, implementation, or realization of a
    product that, if fixed, would eliminate the potential for a failure to be
    observed. Faults are often localized to a specific point, but they can also
    result from the mishandling of a confluence of events that arise during
    product operation.</p>

    <p>The goal of testing is to create conditions that make failures observable. Once a failure is observed, it is the responsibility of developers, or testers in a development role, to debug these failures, locate the faults, and implement fixes.</p>

    <p>Root cause analysis extends beyond the scope of development and test. It
    involves examining project workflows to understand why a fault exists in the
    product. Typically, root cause analysis will identify a root cause that, if
    "fixed," would not eliminate the potential for a failure to be observed in
    the current or near-term releases. Consequently, root cause analysis is
    generally not a priority for design and testing but instead falls within the
    domain of project management.</p>

    <p>A technique commonly used to increase the variety of conditions—and thus the likelihood of creating conditions that reveal faults—is to run more tests with different inputs. This is called increasing the <span class="term">test coverage</span>.</p>

    <p>The Mosaic tool assists testers in finding failures, but it does not directly help with identifying the underlying fault that led to the failure. Mosaic is a tool for testers. However, these two tasks—finding failures and locating faults—are not entirely separate. Knowing where a failure occurs can provide the developer with a good starting point for locating the fault and help narrow down possible causes. Additionally, once a developer claims to have fixed a fault, that claim can be verified through further testing.</p>

    <h2>Testing Objectives</h2>

    <ul>
      <li>
        <strong>Verification Testing</strong><br>
        <em>Purpose</em>: To confirm that the software or system meets the specified requirements and design. Verification testing ensures that each component behaves as expected according to specifications, often conducted throughout development to catch any deviations from the original plan.
      </li>
      
      <li>
        <strong>Regression Testing</strong><br>
        <em>Purpose</em>: To ensure that recent changes or additions to the codebase have not introduced new errors. This type of testing checks that previously tested functionalities still work as intended, making it essential for maintaining stability as updates are made.
      </li>
      
      <li>
        <strong>Development Testing</strong><br>
        <em>Purpose</em>: To evaluate code correctness and functionality during the development process. Development testing is often exploratory, allowing developers to check whether their code performs as expected before formal testing. It can include unit testing, integration testing, and other quick checks to validate functionality on the fly.
      </li>
      
      <li>
        <strong>Exploratory Testing</strong><br>
        <em>Purpose</em>: To uncover unexpected issues by testing the software in an unscripted manner. Exploratory testing allows testers to investigate the software's behavior outside of planned test cases, often discovering edge cases or flaws that structured tests may miss.
      </li>
      
      <li>
        <strong>Performance Testing</strong><br>
        <em>Purpose</em>: To assess how the software performs under expected and extreme conditions. Performance testing evaluates response times, resource usage, and stability, often covering areas like load, stress, and scalability testing. This objective ensures the system can handle the demands it will face in production.
      </li>
      
      <li>
        <strong>Compliance Testing</strong><br>
        <em>Purpose</em>: To confirm that the software adheres to regulatory, legal, and industry standards. Compliance testing ensures that the system meets external requirements, which may include accessibility, data privacy, and industry-specific standards.
      </li>

      <li>
        <strong>Security Testing</strong><br>
        <em>Purpose</em>: To identify vulnerabilities and ensure the software is protected against unauthorized access and threats. Security testing checks for risks like data breaches, weak authentication, and exposure to known vulnerabilities, helping to safeguard sensitive information and user privacy.
      </li>

      <li>
        <strong>Compatibility Testing</strong><br>
        <em>Purpose</em>: To verify that the software works across different environments, devices, and platforms. Compatibility testing ensures consistent functionality and appearance across browsers, operating systems, hardware configurations, and other setups.
      </li>
      
      <li>
        <strong>Acceptance Testing</strong><br>
        <em>Purpose</em>: To determine if the software meets the end user's needs and expectations. Acceptance testing, often conducted by stakeholders or QA teams, validates that the software is usable and functional from a real-world perspective, acting as the final check before release.
      </li>
      
      <li>
        <strong>Documentation Testing</strong><br>
        <em>Purpose</em>: To ensure that all documentation, guides, and user manuals are accurate and reflect the current software functionality. Documentation testing verifies that users have clear, up-to-date information for effective usage and troubleshooting.
      </li>
      
      <li>
        <strong>Usability Testing</strong><br>
        <em>Purpose</em>: To confirm that the software is user-friendly and intuitive. Usability testing focuses on the ease of use, ensuring that end users can navigate and interact with the software without unnecessary friction, leading to a positive user experience.
      </li>
      
    </ul>

    <p>The Moasic Testbench is useful for any type of testing that can be
      formulated as test routines testing RUTs. This certainly includes
      verification, regression, development, exploratory testing. It will
      include the portions of performance, compliance, security, compatibility,
      and acceptance testing that fit the model of test routines and RUTs.  Only
      recently has can it be imagined that the Mosaic TestBench can be used with
      documentation testing. However, it is now possible to fit an AI API into a
      test routine, and turn a document into a RUT.  Usability testing often
      depends in other types of tests, so to this extent the Mosaic Testbench
      can play a role. However, usability is often also in part feedback from
      users. So short of putting users in the Matrix, this portion of usability
      testing remains outside the domain of the Mosaic Testbench, though come to
      think of it, the Mosaic Testbench could be used to reduce surveys to pass
      fails.</p>
      
    <p>Each test objective will lead to writing tests of a different nature.</p>
    

    <h2>Unstructured Testing</h2>

    <p>This section outlines some common approaches
      to <span class="term">unstructured testing</span>, often referred to
      as <span class="term">black box testing</span>. Black boxes are inherent
      in even the most structured testing approaches, as at the lowest levels of
      analysis, elements will always remain opaque. Even in the most highly
      detailed test of logic possible, one that examines a RUT down to the
      individual logic gates, each gate would be treated as a black box.</p>

    <h3>Reference Output Based Testing</h3>

    <p>In <span class="term">reference output based testing</span>, an ordering
      is assigned to the <span class="term">inputs</span> for
      the routine under test, as well as to
      its <span class="term">outputs</span>. Through this ordering the inputs
      and outputs become vectors. Thus the routine under test is given
      an <span class="term">input vector</span> and it returns
      an <span class="term">observed output vector</span>.</p>

    <p>A <span class="term">Reference Model</span> is then 
      given the same input vector, and then it
      produces a <span class="term">reference output vector</span>. The reference
      output vector has the same component ordering as the 
      <span class="term">observed output vector</span>.

    <p>The <span class="term">failure detection function</span> then compares
    each observed output vector with its corresponding reference output vector. If
    they do not match, the test is deemed to have failed.</p>

    <p>It follows that in reference output based testing, the accuracy of the
    test results depends solely on the accuracy of the Reference Model.</p>

    <p>When the implementation of the Reference Model is unrelated to the
      routine under test, we tend to expect that the errors produced by the
      Reference Model will be uncorrelated with those produced by the routine
      under test, and thus not probable to coincide. This property will bias
      test routines towards delivering false fails. As noted earlier, false fails are
      likely to be caught as test fails are followed up with further
      scrutiny. It follows that reference output based testing can potentially
      deliver a high degree of accuracy even though the reference model is not
      ideal.</p>

    <h3>Property Check Testing</h3>

    <p><span class="term">Property Check Testing</span> is an alternative to
    reference output based testing. Here, rather than comparing each observed
    output to a reference output, the observed output is validated against
      known properties or expected characteristics.</p>

    <p>For example, given an integer as input, a function that correctly squares
      this input will preserve the parity of the input, as an odd number squared
      will be odd, and an even number squared will be even. The failure decider
      can check this property for each test case, and if it does not hold, the
      test case fails.</p>

    <p>Note for the square RUT test, this proposed property check is weak. Given
      a uniform distribution, half the time an errant square will still have the
      correct parity. There are stronger property checks that could be done for
      squares, but the point here is one of illustration.  A weak property check
      would not recognize many failures, and thus be biased towards false pass
      decisions. Those are the bad ones, as passing tests typically receive no
      further scrutiny.</p>

    <h3>Spot Checking</h3>

    <p>In spot checking, the function under test is checked against one or two
      input vectors. When using a black box approach, these are chosen at
      random.</p>

    <p>Moving from zero to one is an finite relative change, i.e., running a
      program for the first time requires that many moving parts work together,
      parts that have never been tried before; hence, a tremendous amount is
      learned about the logic and setup when the first test runs. Such a first
      test is called a <scan class="term">smoke test</scan>, a term that
      has literal meaning in the field of electronics testing.</p>

    <h3>Exhaustive Testing</h3>

    <p>A test routine will potentially run multiple test cases against a given
      RUT. If the RUT is a pure function, then per test case, a single test
      vector will be given to the RUT, and a single output vector will be
      returned.  However, if the RUT is sequential in nature, for each test case
      there will be a sequence of input vectors, and potentially a sequence of
      output vectors.</p>

    <p>The set of possible inputs for a RUT, were members are either individual
      vectors, or vector sequences, constitutes the <span class="term">input
      space</span>. Test <span class="term">coverage</span> is typically given
      as the proportion or inputs tested to the total in the input space,
      reported as a percentage./p>

    <p>When the RUT is a pure function, the input space is an enumeration of all
      possible input vectors. If the inputs include arbitrary long strings, then it
      will not be possible to complete such an enumeration, the best that can
      be done is to generate more and more inputs upon demand.
    </p>

     <p>When the RUT has sequential behavior, achieving full coverage requires
      giving the RUT every possible starting input, and then sequencing it to a
      point of hitting a stop state or cycle state in every possible way.  Again
      if inputs can be arbitrarily long strings, such an enumeration can not be
      completed. Furthermore, if the RUT state is encapsulated unseen in a black
      box, it might be very difficult, or impossible, to detect when the state
      has cycled.</p>

    <p><span class="term">Exhaustive testing</span> is said to have been
      done when every single input in the input space has been tested.
      An exhaustive test will have obtained 100% coverage, with no rounding
      done in the coverage computation.</p>

    <p>Suppose that a fault appears at time t₀. Suppose there is a duration of
      time of interest, Δ, that begins at or later than t₀. Suppose further
      there exists a given test and test case that fails due to the fault, but
      would not otherwise fail. Then a <span class="term">failure is
      reproducible</span> during Δ, if and only if the given test and test case
      would fail if run at any time during Δ, and no matter how many times it is
      run.</p>

    <p>For a RUT that is a pure function, this definition is the same as saying
      the test case fails at the same input value every time during Δ, when
      ideally is should have passed. For a sequential RUT, it is saying that the
      same input vector sequence will always lead to a failure, when ideally it
      would lead to a pass.</p>

    <p>Although the same test routine is run with identical inputs, a failure
      might not be reproducible due to other sources of variability, as
      examples:</p>
    <ol>
      <li>The contract made with the programmer for using the exact same
        inputs for the exact same test routine was broken.
      <li>Use of uninitialized memory.
      <li>Software updates or platform changes in between test runs during Δ.
      <li>Green thread, or real thread, scheduling differences, whether done by the OS or by the interpreter.
      <li>Using the system time as data, or other system parameter.
      <li>Race conditions.
      <li>Getting values from a randomly seeded pseudo random number generator.</li>
      <li>Reaching out of the architecture model for values, as examples
        using performance measures or by timing events.</li>
      <li>A hardware fault that is sensitive to a myriad of possible environmental
        influences.</li>
    </ol>
    
    <p>Exhaustive testing will find all failures that are reproducible. It might
      find failures that are not reproducible. The probability of witnessing
      non-reproducible failures will typically go up when using the technique
      of <span class="term">over testing</span>, i.e. running even more than an
      exhaustive number of tests.</p>

    <h2>Structured Testing</h2>

    <p>Structured testing is a form of white box testing, where the tester
    examines the code being tested and applies various techniques to it
    to increase the efficiency of the testing.</p>

    <h3>The Need for Structured Testing</h3>

    <p>All types of black-box testing have a serious problem in that the search
      space for failures grows exponentially as the number of inputs grows. You have
      probably heard about this sort of thing before, but you might not appreciate
      just how severe the situation is. To illustrate, we will consider the simplest of 
      programs, one that adds two numbers. When the RUT is a black box, the test routine 
      only has access to the interface, so it appears like this:</p>

    <pre><code>
        int8 sum(int8 a, int8 b){
        ...
        }
    </code></pre>
    
    <p>Here, two <code>int8</code> values are being added, so an input test vector will have 
      16 bits. The result is also an <code>int8</code>, so an output vector will have 8 bits.</p>

    <p>As the internals of the RUT are unknown, it could contain unexpected logic, like this:</p>

    <pre><code>
        int8 sum(int8 a, int8 b){
        if(a == 248 && b == 224) return 5;
        else return a + b;
        }
    </code></pre>
    
    <p>A developer might not be writing malicious code when something like this
      appears; instead, the code might have been pulled from somewhere else and
      dropped in. There could have been a special case in this situation on another
      machine. Perhaps the code was generated by an AI, or it could be leftover
      debug information. This example illustrates that testers are typically not
      responsible for understanding developer code. Though in this case the logic
      is obvious, there can be more obscure functions that testers cannot take the
      time to understand, which might exhibit similar unexpected behavior.</p>

    <p>As this is a black box, the numbers 248 and 224 are not known to the test writer. 
      Therefore, the only effective unstructured testing approach that is guaranteed to 
      find this failure is exhaustive testing.</p>

    <p>Exhaustive testing is feasible here. An input test vector with 16 bits will lead to 
      an input space of 65,536 points. Sixty-five thousand tests is trivial for a modern 
      desktop. The full test will take about 100 microseconds, and in this time the test 
      routine is guaranteed to find all failures. Note that in 50 microseconds, half of 
      the input space will be covered, so there is a 0.5 probability of finding a single 
      failure within that time. Generally, half the total time corresponds to a 0.5 probability 
      of finding a single failure.</p>

    <p>Now, suppose that instead of looking for a reproducible fault, we have:</p>
    <pre><code>
      int8 sum(int8 a, int8 b){
        if(a == 255 * rand() && b == 224 * rand()) return 5;
        else return a + b;
      }
    </code></pre>

    <p>In this case, to find the fault, the test routine must guess the values of two independent 
      8-bit random variables from a uniform distribution. As they are independent, we can combine 
      them and note that the test must guess a 16-bit value. If we consider an "exhaustive" test, 
      the tester will make 2^16 tries. Hence, the probability of finding this failure is:</p>

    <pre><code>
        1 - (1 - 2<sup>-16</sup>)<sup>2<sup>16</sup></sup> = 0.6321...
    </code></pre>

    <p>A small adjustment to the above equation is necessary to make it precise, because 
      sometimes 5 is the correct answer. Thus, with 2<sup>16</sup> test cases, there will 
      be certainty (a probability of 1.0) in finding all reproducible errors and about 
      a 0.63 probability of finding a single random fault. The two probabilities are not 
      as far apart as one might expect, given that the failure is "jumping around."</p>

    <p>Now, let's go back to the reproducible error case, but this time, suppose we are working 
      with an <code>int16</code>:</p>

    <pre><code>
      int16 sum(int16 a, int16 b){
        ...
      }
    </code></pre>
    
    <p>Now an input vector has 32 bits, giving an input space with 21,474,836,480 points. 
      Our computer will require about 33 seconds of compute time for this. Adding around 
      10 seconds for wall-clock time, let’s call it 40 seconds. Testing would be barely 
      practical if it took 40 seconds to test such a simple RUT as this, but perhaps we 
      would invest in a faster computer?</p>

    <pre><code>
      int32 sum(int32 a, int32 b){
        ...
      }
    </code></pre>

    <p>Now, suppose we are adding 32-bit numbers. The input space now has 18,446,744,073,709,551,616 points. 
      Compute time, without overhead, will be about 4,496 years! Suffice it to say, we have discovered that 
      testing the addition of two 32-bit numbers exhaustively is impractical. Even if we break the problem 
      into 1,000 pieces on different processors and use a state-of-the-art server farm, it would still take 
      months and cost a significant amount. What will you tell the boss?</p>

    <p>But wait! What if we move to 64-bit computing?</p>

    <pre><code>
        int64 sum(int64 a, int64 b){
        ...
        }
    </code></pre>
    
    <p>The input space now has:</p>
    <pre><code>
        340,282,366,920,938,463,463,374,607,431,768,211,456
    </code></pre>
    <p>points. That's about 340 undecillion. Compute time is 83 sextillion years—or about 
      6 trillion times the age of the universe. Even with all the processing power on Earth, 
      even if you're willing to accept a probability of 0.1 of finding the failure, it would 
      take a thousand times longer than the age of the universe to test a function as simple 
      as adding two numbers. Clearly, there must be a better approach.</p>


    <h4>Summary Table</h4>

    <table>
      <tr>
        <th>Bits</th>
        <th>Input Space</th>
        <th>Compute Time</th>
      </tr>
      <tr>
        <td>8 bits</td>
        <td>6.55 x 10<sup>4</sup></td>
        <td>100 μs</td>
      </tr>
      <tr>
        <td>16 bits</td>
        <td>2.15 x 10<sup>10</sup></td>
        <td>33 s</td>
      </tr>
      <tr>
        <td>32 bits</td>
        <td>1.84 x 10<sup>19</sup></td>
        <td>4,496 years</td>
      </tr>
      <tr>
        <td>64 bits</td>
        <td>3.40 x 10<sup>38</sup></td>
        <td>6 x 10<sup>12</sup> times the age of the universe</td>
      </tr>
    </table>

    <p>A typical response from people when they see this is that the knew it went up
      fast, but did not know it went up this fast. It is also important to note, there
      is a one to one relationship between percentage of time to achieving exhaustive
      coverage, and percentage of coverage.  Half the time, 50 percent coverage. In
      the last row of the table, to have reasonable test times, there would be coverage
      10<sup>-18</sup> percentage coverage. At that level of coverage there is really
      no reason to test. Hence, this table is not limited to speaking about exhaustive
      testing, rather it speaks to black box testing in general.</p> 

    <h3>Informed Spot Checking</h3>

    <p>In white box testing, we take the opposite approach to black box
       testing. The test writer does look at the code implementation and
      must understand how to read the code. Take our 64-bit adder example of
      the prior section. Here in this section we will apply a white box
      technique known as Informed Spot Checking.</p>

    <p> This is the prior example as a black box:</p>

    <pre><code>
      int64 sum(int64 a, int64 b){
        ...
      }
    </code></pre>

    <p>And here it is as a white box:</p>

    <pre><code>
      int64 sum(int64 a, int64 b){
        if(a == 5717710 && b == 27) return 5;
        else return a + b;
      }
    </code></pre>

    <p>When following the approach of Informed Spot Checking, the tester examines
       the code and sees there is a special case for <code>a = 5717710</code>
       and <code>b = 27</code>, which becomes the first test case. There’s also
       a special case for when the sum exceeds the 64-bit integer range, both in
       the positive and negative directions; these become two more test
       cases. Finally, the tester includes a few additional cases that are not
       edge cases.</p>

    <p>Thus, by using white box testing instead of black box testing, the tester finds all 
       the failures with just 4 or so test cases instead of </p>
    <pre><code>
      340,282,366,920,938,463,463,374,607,431,768,211,456 
     </code></pre>
       <p>cases. Quite a savings, eh?</p>

    <p>There are notorious edge cases in software, and these can often be seen
      by looking at the RUT. Zeros and inputs that lead to index values just off
      the end of arrays come to mind are common ones. Checking a middle value
      and edge cases is often an effective approach for finding failures.</p>

    <p>There is an underlying mechanism at play here. Note that it takes two
      points to determine a line. In Fourier analysis, it takes two samples per
      period of the highest frequency component to determine an entire
      waveform. Code also has patterns, patterns that are disjoint at edge
      cases. Hence if a piece of code runs without failures for both edge cases
      and spot check values in between, it will often run without failures over
      an entire domain of values. This effect explains why ad hoc testing has
      lead to so much relatively fail free code.</p>

    <p>Informed Spot Checking is especially valuable in early development, as it
      provides useful insights with minimal investment. In the early development
      stage, making more investment in test code is unwise due to the code being
      in flux. Test work is likely to get ripped up and replaced.</p>

    <p>The idea of test work being ripped up and replaced highlights a drawback
      of white box testing. Analysis of code can become stale when implementations
      are changed. However, due to the explosion in the size of the input space
      with even a modest number of inputs, white box testing is necessary if there
      is to be much commitment to producing reliable software or hardware.</p>

    <h3>Refactoring the RUT</h3>

    <p>Refactoring a RUT to make it more testable can be a powerful method for
      turning testing problems that are exponentially hard due to state
      variables, or very difficult to debug due to random variables, into
      problems that are linearly hard. According to this method, the
      tester is encouraged to examine the RUT to make the testing problem
      easier.</p>

    <p>By reconstructing the RUT I mean that we refactor the code to bring
      any random variables or state variables to the interface where they
      are then treated as inputs and outputs.</p>

    <p>If placing state variables on the interface is adopted as a discipline by
      the developers, reconstruction will not be needed in the test phase, or if
      it is needed, white box testers will see this, and it will be a bug that
      has been caught. Otherwise reconstruction leads to two versions of a
      routine, one that has been reconstructed, and the other that has not. The
      leverage gained on the testing problem by reconstructing a routine
      typically more than outweighs the extra verification problem of comparing
      the before and after routines.</p>

    <p>As an example, consider our adder function with a random fault. As we
      know from prior analysis, changing the fault to a random number makes
      testing harder, but perhaps more importantly, it makes it nearly impossible
      to debug, as the tester can not hand it to the developer and say,
      'it fails in this case'.</p>
    <pre><code>
      int64 sum(int64 a, int64 b){
        if( a == (5717710 * rand()) && b == (27 * rand()) ) return 5;
        else return a + b;
      }
    </code></pre>

    <p>The tester refactors this function as:</p>
    <pre><code>
      int64 sum( int64 a, int64 b, a0 = 5717710*rand() ,b0 = 27*rand() ){
        if( a == a0 && b == b0 ) return 5;
        else return a + b;
      }
    </code></pre>
    
    <p>Here <code>a0</code> and <code>b0</code> are added to the interface as
      optional arguments. During testing their values will be supplied, during
      production the defaults will be used. Thus, we have broken the one
      test problem into two, the question if <code>sum</code> works, and the
      question if the random number generation works.<p>

    <p>Failures in <code>sum</code> found during testing are now reproducible.
      If the tester employs the informed spot checking the failure will
      be found with few tests, and the point in the input space where the
      failure occurs can be reported to development and used for debugging.</p>
    
    <p>Here is a function that keeps a state variable between calls.</p>
    <pre><code>
    int state = 0;
    int call_count = 0; 
    void state_machine(int input) {
        int choice = (input >> call_count) & 1; 
        switch (state) {
            case 0:
                printf("State 0: Initializing...\n");
                state = choice ? 0 : 1;
                break;
            case 1:
                printf("State 1: Processing Path A...\n");
                state = choice ? 0 : 2; 
                break;
            case 2:
                printf("State 2: Processing Path B...\n");
                state = choice ? 0 : 3;
                break;
        }
        call_count++;
    }
    </code></pre>

    <p>The Mosaic Testbench makes standard out available to the test routine in
      an array so we can capture and examine the print value while testing this
      RUT. Because of the state variables, <code>state</code>
      and <code>count</code>, this routine will behave differently each time it
      is called. A black box test will have a large number of input vector
      sequences to try. The failure occurs in the call after being in state 2
      and the count is such that the choice is to go to state 3.</p>

    <pre><code>
    int state = 0;
    int call_count = 0; 
    void state_machine(int input ,int state0 = state ,int call_count0 = call_count) {
        int choice = (input >> call_count0) & 1; 
        switch (state0) {
            case 0:
                printf("State 0: Initializing...\n");
                state = choice ? 0 : 1;
                break;
            case 1:
                printf("State 1: Processing Path A...\n");
                state = choice ? 0 : 2; 
                break;
            case 2:
                printf("State 2: Processing Path B...\n");
                state = choice ? 0 : 3;
                break;
        }
        call_count = call_count0 + 1;
    }
    </code></pre>

    <p>Here the test routine supplies <code>state0</code> and <code>call_count0</code>
      as inputs.  The test routine treats <code>state</code> and <code>call_ccount</code>
      as outputs, so this is then a pure function. As a pure function it is a much easier
      testing problem. Now instead of a combinatorially hard problem involving input
      sequences, the test routine can visit each of the three states, and set the input
      such that each visits the two next states. That is six test cases to see everything
      that this function is capable of doing.</p>

    <p>Any time the RUT is refactored in the testing phase, it raises the
      question if the refactored code maintains the required functionality.
      This becomes another verification problem, which might or might not
      be verified through testing. One way to manage this issue is to
      take the refactoring problems back to the developers to have them
      adopt the code into the project. Then it becomes the original code.</p>
   
    <h3>Bottom Up Testing</h3>

    <p>When a function corresponds directly to CPU instructions, such as is the
      case for the <code>+</code> operator, we typically trust that it will give
      the right answer. The same can be said for the call and return
      dynamic. Unless we are working on a new compiler, it is typically assumed
      that this works. Tests for it are not included for testing if calls work in
      application program test suites.
      </p>

    <p>The reason for this trust is that CPU instructions, and function calls
      are already extensively tested, both directly by the manufacturers, and
      through widespread use. Though this trust is not always warranted as in
      the case of the Intel Pentium divider, which had failure cases.</p>

    <p>We can decompose a testing problem into trusted and untrusted components.
       We call routines that are trusted <span class="term">building blocks</span>,
       then we use the building blocks to build up larger routines, and then
       test those to create larger building blocks. At the end we will have
       built up a trustworthy program.</p>

    <p>This approach parallels what developers do when they write programs. They
      start with primitive programs that come with the language or from
      libraries, and then they compose these to write custom functions.</p>

    <p>The following is an expansion of our adder example for creating and
      testing an adder for 1024 bit numbers. For purposes of presentation, we
      will refer to <code>int256</code> as a type that corresponds to array of
      32 bytes, and <code>uint1</code> as a 1 bit unsigned integer, i.e. 0 or
      1.</p>

    <pre><code>
    {uint1, uint64} full_adder(uint64 a, uint64 b, uint1 c0) {
        uint64 partial_sum = a + b;
        uint64 sum = partial_sum + c0;
        uint1 carry_out = (partial_sum < a) || (sum < partial_sum);
        return {carry_out, sum};
    }
    </code></pre>

    <p>Here is a 256 bit adder made from 64 bit adders.</p>

    <pre><code>
    {uint1, int256} add_256(int256 a, int256 b) {
        uint1 carry_in = 0;
        int64 sum_parts[4];  // Array to store each 64-bit segment of the sum

        for i = 0 to 3 {
            // Get the i-th 64-bit segments of a and b
            int64 a_part = (a >> (i * 64)) & 0xFFFFFFFFFFFFFFFF;
            int64 b_part = (b >> (i * 64)) & 0xFFFFFFFFFFFFFFFF;

            // Perform the full addition on each 64-bit part
            {carry_out, sum_parts[i]} = full_adder(a_part, b_part, carry_in);

            // Update carry-in for the next 64-bit segment
            carry_in = carry_out;
        }

        int256 sum = 0;
        for i = 0 to 3 {
            sum |= (sum_parts[i] << (i * 64));
        }

        return {carry_in, sum};
    }
    </code></pre>
    
    <p>According to the bottom up technique, we first test
    the <code>full_adder</code>, which is not a difficult testing problem. It
    employs well known trusted operations, and has a couple of interesting
    special case conditions. Given the numeric nature of this code, these
    special case conditions are probably better verified by proof than by
      testing, but they can be tested.</p>

    <p>Once the <code>full_adder</code> can be trusted, testing <code>add_256</code>
      reduces to checking that the various 64 bit parts are extracted and then
      packed correctly,
      and are not, say, offset by one, and that the carries are properly communicated
      during the add.</p>

    <p>Note this test also trusts the fact that ripple carry addition is a valid
      algorithm for assembling the pieces. Thus there is a new verification
      problem, that for the algorithm. In this case, ripple carry addition is
      already a trusted algorithm.</p>

    <p>Testing of <code>full_adder</code> could be further simplified with
      refactoring, by moving the loop control variables to the interface and the
      <code>carry_in</code> and <code>carry_out</code> to the interface.
      As <code>i</code> is recycled, it would become two variables,
      say <code>i</code> and <code>j</code>.  Once the loop control variables
      are on the interface it is straight forward to test the packing. Once the
      carries are on the interface it is straight forward to test the
      carries.</p>

    <p>In general all programs and circuits can be conceptualized as functional
      units, channels, and protocols. A test that shows that these work as specified,
      shifts the test problem from the RUT to the specification.</p>

    <h2>Adding to the code</h2>

    <p>It is a common practice to add property checks to the code for gathering
      data about failures or other potential problems. These will then write to
      log files, or even send messages back to the code maintainers. By doing
      this the testers benefit from the actual use of the product as though it
      were a test run. When failures are found, such code might then trigger
      remedial or recovery actions.</p>

<h2>About Reference Outputs and Reference Properties</h2>

<p>When testing during development, reference outputs often come from the
   developers or testers themselves. They know what they expect from the 
   routines, but they do not know if the code will meet these expectations, 
   so they write tests. Typically, they try to imagine the hardest possible 
   cases. However, sometimes a young developer avoids testing challenging 
   cases to sidestep the risk of failures—this is, of course, a poor approach 
   that can lead to undetected issues.</p>

<p>Often, specification authors provide reference outputs or extensive test 
   suites that must be passed to achieve certification. Architects also 
   contribute by creating multi-level specifications—for the entire program, 
   for the largest components, and for communication protocols between 
   components. These specifications often serve as high-quality reference 
   outputs and property checks that can be applied to the model during testing. 
   The goal of developers and testers is to meet these specifications, making 
   failures directly relevant to the development process and program design.</p>

<p>Experts in a specific area sometimes provide test data, maintaining 
   a database of reference data as a resource for validating outputs. 
   For some types of code, experts also supply property checks, which 
   evaluate whether outputs satisfy essential properties rather than specific 
   values. Depending on the domain, these properties can be an important aspect 
   of the testing process.</p>

<p>Each time a bug is found, a test should be created to capture a failure
   related to that bug. Ideally, such tests are written with minimal
   implementation-specific details so they remain relevant even after code
   changes. These tests are then added to a regression testing suite, ensuring
   that future changes do not reintroduce the same issues.</p>

<p>For applications involving multi-precision arithmetic, such as the earlier
   adder example, reference data is often sourced from another established
   multi-precision library, whether an open-source or commercial product. The
   assumption is that an existing product will be more reliable than a newly
   developed one, and since it’s implemented differently, its errors are likely
   to be uncorrelated. This competitive testing, which is aspect of
   compatibility testing, here being used for other objectives. In the limit, as
   the RUT matures, this approach will tend to identify bugs in the reference
   data from the other company as often it does in the RUT, which might be an
   interesting effect.</p>

<p>In some cases, reference data comes from historical sources or existing 
   systems. When upgrading or replacing a legacy system, historical data 
   serves as a benchmark for comparison. Similarly, industry standards 
   and compliance datasets, particularly from regulatory organizations 
   like IEEE, NIST, or ISO, provide reliable reference points for applications 
   requiring standardized outputs. Compliance-driven tests are often required 
   for certification or regulatory approval in fields such as finance, 
   healthcare, and aerospace.</p>

<p>For cases requiring many inputs without needing specific reference values,
  random number generators can provide extensive test data. Examples include in
  comparative testing and when property checking. Random number generators can
  also be configured to concentrate cases in specific areas of the input domain
  that for some reason concerns the testers.</p>

<p>Customer and user feedback sometimes uncovers additional test cases, 
   especially when dealing with complex or evolving software. Feedback 
   reveals edge cases or expected behaviors that developers and testers 
   may not have anticipated, allowing teams to create reference points 
   for new test cases that cover real-world use cases and address user needs.</p>

<h2>Conclusion</h2>

<p>If you are a typical tester or developer reading through the previous list, 
   you might feel a bit disappointed. Unless you work in a specialized area, 
   are attempting to create a compatible product, or need to exercise the hardware, much 
   of that list might seem inapplicable. For many developers, the most 
   applicable advice remains: "During development, reference outputs often 
   come from the developers or testers themselves." I apologize if this seems 
   limiting, but consider this: the reason we run programs is to generate the 
   very data we're looking for. If that data were easily available, we wouldn’t 
   need the program.</p>

<p>In many ways, testing is about making developers and testers the first 
   users of the product. All products will have bugs; it’s far better for 
   experts to encounter these issues first.</p>

<p>Testing also facilitates communication among project members. Are the 
   architects, developers, and testers all on the same page about how the 
   product should work? The only way to find out is to run what has been built 
   and observe it in action. For this, we need test cases.</p>

<p>This circular problem—finding data that our program should generate - to test 
   the program itself — illustrates a fundamental limitation in software testing. 
   We encountered this in the discussion on unstructured, black-box testing: as 
   soon as we open the box to inspect the code, we are no longer just testing it, 
   but reasoning about it and even verifying it formally.</p>

<p>This, perhaps, hints at a way forward. Our program is a restatement of the 
   specification in another language. Verification, then, is an equivalence 
   check. We can run examples to demonstrate equivalence, but black-box testing 
   alone will have limited impact. Alternatively, we can examine our code and 
   try to prove that it matches the specification. Though challenging, this 
   approach is far more feasible than waiting ten times the age of the universe 
   to confirm our solution through black box testing.</p>

<p>Think of testing as a reasoning problem. Explain why the routine works and 
   how it contributes to meeting the specification. Work from the top down: if 
   the high-level components behave correctly, the program will meet the 
   specification. That’s the first step. Then explain why the breakdown of 
   those top-level components ensures correct behavior. Continue this process, 
   and then use tests to validate each link in this chain of reasoning. In this 
   way, you can generate meaningful reference values.</p>

  </div>
</body>
</html>
  
<!--  LocalWords:  decider's sextillion Testbench
 -->

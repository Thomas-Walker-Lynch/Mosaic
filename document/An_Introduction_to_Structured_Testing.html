<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP&display=swap" rel="stylesheet">
  <title>White Box Testing - Mosaic Project</title>
  <style>
    body {
      font-family: 'Noto Sans JP', Arial, sans-serif;
      background-color: hsl(0, 0%, 0%);
      color: hsl(42, 100%, 80%);
      padding: 2rem;
    }
    .page {
      padding: 3rem; /* 20px */
      margin: 1.25rem auto; /* 20px */
      max-width: 46.875rem; /* 750px */
      background-color: hsl(0, 0%, 0%);
      box-shadow: 0 0 0.625rem hsl(42, 100%, 50%); /* 10px */
    }
    h1 {
      font-size: 1.5rem;
      text-align: center;
      color: hsl(42, 100%, 84%);
      text-transform: uppercase;
      margin-top: 1.5rem;
    }
    h2 {
      font-size: 1.25rem;
      color: hsl(42, 100%, 84%);
      text-align: center;
      margin-top: 2rem;
    }
    h3 {
      font-size: 1.125rem;
      color: hsl(42, 100%, 75%);
      margin-top: 1.5rem;
    }
    p, li {
      color: hsl(42, 100%, 90%);
      text-align: justify;
      margin-bottom: 1rem;
    }
    .term {
      font-family: 'Courier New', Courier, monospace;
/*      background-color: hsl(0, 0%, 19%); */
      padding: 0.125rem 0.25rem;
      border-radius: 0.125rem;
      text-decoration: underline;
/*      font-style: italic;*/
      color: hsl(42, 100%, 95%);
    }
    code {
      font-family: 'Courier New', Courier, monospace;
      background-color: hsl(0, 0%, 25%);
      padding: 0.125rem 0.25rem;
      color: hsl(42, 100%, 90%);
    }
  </style>
</head>
<body>
  <div class="page">
    <header>
      <h1>An Introduction to Structured Testing</h1>
      <p>© 2024 Thomas Walker Lynch - All Rights Reserved.</p>
    </header>


    <h2>Introduction</h2>

    <p>This guide provides a general overview of testing concepts. It is
      not a reference manual for the Mosaic test bench itself. At the
      time of writing, no such reference document exists, so developers and
      testers are advised to consult the source code directly for implementation
      details. A small example can be found in the <code>Test_MockClass</code>
      file within the tester directory. Other examples can be found in projects
      that make use of Mosaic.</p>

    <p>A typical testing setup comprises three main components:
    the <span class="term">test bench</span>, the <span class="term">test
    routines</span>, and a collection of <span class="term">units under
    test</span> (UUTs). Here, a UUT is any individual software or hardware
    component intended for testing. Because this guide focuses on software, we
    use the term <span class="term">RUT</span> (routine under test) to denote
    the unit under test in software contexts. Although we use software-centric
    terminology, the principles outlined here apply equally to hardware
    testing.</p>

    <p>Each test routine supplies inputs to a RUT, collects the resulting
      outputs, and determines whether the test passes or fails based on those
      values. A given test routine might repeat this procedure for any number
      of <span class="term">test cases</span>. The final result from the test
      routine is then relayed to the test bench. Testers and developers write
      the test routines and place them into the test bench.</p>

    <p>Mosaic is a test bench. It serves as a structured environment for
    organizing and executing test routines, and it provides a library of utility
    routines for assisting the test writer. When run, the test bench sequences
    through the set of test routines, one by one, providing each test routine
    with an interface to control and examine standard input and output. Each 
    test routine, depending on its design, might in turn sequence through
    test cases. During execution, the test
    bench records pass/fail results, lists the names of the test routines that failed,
    and generates a summary report with pass/fail totals.</p>

    <p>At the time of this writing, Mosaic does not provide features for
      breaking up large test runs into parallel pieces and then load balancing
      those pieces. Perhaps such a feature will be developed for a future version.
      However, this does not prevent an enterprising tester from running multiple
      Mosaic runs with different test routines in parallel in an ad hoc manner, or
      with other tools.</p>

    <h2>Function versus Routine</h2>

    <p>A routine is an encapsulated sequence of instructions, with a symbol
      table for local variables, and an interface for importing and exporting
      data through the encapsulation boundary. This interface
      maps <span class="term">arguments</span> from a caller
      to <span class="term">parameters</span> within the routine, enabling data
      transfer at runtime. In the context of testing, the arguments that bring
      data into the routine are referred to as
      <span class="term">inputs</span>, while those that carry data out are called
      <span class="term">outputs</span>. Notably, in programming, outputs are often called 
      <span class="term">return values</span>.</p>

   <p>In computer science, a <span class="term">pure function</span> is a routine
     in which outputs depend solely on the provided inputs, without reference to
     any internal state or memory that would persist across calls. A pure function
     produces the same output given the same inputs every time it is called. 
     Side effects, such as changes to external states or reliance on external
     resources, are not present in pure functions; any necessary interactions 
     with external data must be represented explicitly as inputs or outputs. 
     By definition, a function produces a single output, though this output can 
     be a collection, such as a vector or set.</p>

    <p>Routines with internal state variables that facilitate temporal behavior
      can produce outputs that depend on the sequence and values of prior
      inputs. This characteristic makes such routines challenging to
      test. Generally, better testing results are achieved when testing pure
      functions, where outputs depend only on current inputs.</p>


    <h2>Block and Integration</h2>

    <p>A test routine provides inputs to a RUT and collects its outputs, often
    doing so repeatedly in a sequence of test cases. The test routine then
    evaluates these values to determine if the test has passed or failed.</p>

    <p>When a test routine evaluates a RUT that corresponds to a single function
    or module within the program, it performs a <span class="term">block
    test</span>.</p>

    <p>When a test routine evaluates a RUT that encompasses multiple program
    components working together, it is conducting
    an <span class="term">integration test</span>.</p>

    <p>Integration tests typically involve combining substantial components of a
    program that were developed independently. Such tests can occur later in the
    project timeline, where they can reveal complex and unforeseen interactions
    between components when there is not adequate time to deal with them. To
    help address these challenges, some software development methodologies
    recommend to instead introducing simplified versions of large components
    early in the development process, and to then refine them over time.</p>

    <h2>Failures and Faults</h2>

    <p>A test routine has two primary responsibilities: firstly in supplying inputs
      and collecting outputs from the RUT, and secondly in determining whether the RUT
      passed or failed the test. This second responsibility is handled by
      the <span class="term">failure decider</span>. When the failure decider is not
      an explicit function in the test routine,its functionality will still be present
       in the test routines logic.</p>

    <p>A given failure decider might produce <span class="term">false
    positive</span> or <span class="term">false negative</span> results. A
    false positive occurs when the failure decider indicates that a test has
    passed when it should have failed; hence, this is also known as
    a <span class="term">false pass</span>. Conversely, a false negative occurs
    when the decider indicates failure when the test should have passed; hence, this also
    known as a <span class="term">false fail</span>. An <span class="term">ideal
    failure decider</span> would produce neither false passes nor false
    fails.</p>

    <p>In a typical testing workflow, passing tests receive no further
    scrutiny. In contrast, failed tests are further examined to locate the
    underlying fault. Thus, for such a workflow, false fails are likely to be
    caught in the debugger, while false passes might go undetected until
    release, then be discovered by users. Early in the project timeline, this
    effect can be mitigated by giving passing cases more scrutiny, essentially
    spot-checking the test environment. Later, in regression testing, the volume
    of passing cases causes spot-checking to be ineffective. Alternative
    strategies include redundant testing, better design of the failure decider,
    or employing other verification techniques.</p>

    <p>A failure occurs when there is a deviation between the <span class="term">observed output</span> from a RUT and the <span class="term">ideal output</span>. When the ideal output is unavailable, a <span class="term">reference output</span> is often used in its place. When using reference outputs, the accuracy of test results depends on both the accuracy of the failure decider and the accuracy of the reference outputs themselves.</p>

    <p>Some testers will refer to an <span class="term">observed output</span> as an <em>actual output</em>. Additionally, some testers will call <span class="term">reference outputs</span> <em>golden values</em>, particularly when those values are considered highly accurate. However, the terminology introduced earlier aligns more closely with that used in scientific experiments, which is fitting since testing is a form of experimentation.</p>

    <p>A fault is a flaw in the design, implementation, or realization of a
    product that, if fixed, would eliminate the potential for a failure to be
    observed. Faults are often localized to a specific point, but they can also
    result from the mishandling of a confluence of events that arise during
    product operation.</p>

    <p>The goal of testing is to create conditions that make failures observable. Once a failure is observed, it is the responsibility of developers, or testers in a development role, to debug these failures, locate the faults, and implement fixes.</p>

    <p>Root cause analysis extends beyond the scope of development and test. It
    involves examining project workflows to understand why a fault exists in the
    product. Typically, root cause analysis will identify a root cause that, if
    "fixed," would not eliminate the potential for a failure to be observed in
    the current or near-term releases. Consequently, root cause analysis is
    generally not a priority for design and testing but instead falls within the
    domain of project management.</p>

    <p>A technique commonly used to increase the variety of conditions—and thus the likelihood of creating conditions that reveal faults—is to run more tests with different inputs. This is called increasing the <span class="term">test coverage</span>.</p>

    <p>The Mosaic tool assists testers in finding failures, but it does not directly help with identifying the underlying fault that led to the failure. Mosaic is a tool for testers. However, these two tasks—finding failures and locating faults—are not entirely separate. Knowing where a failure occurs can provide the developer with a good starting point for locating the fault and help narrow down possible causes. Additionally, once a developer claims to have fixed a fault, that claim can be verified through further testing.</p>


    <h2>Unstructured Testing</h2>

    <p>This section outlines some common approaches
      to <span class="term">unstructured testing</span>, often referred to
      as <span class="term">black box testing</span>. Black boxes are inherent
      in even the most structured testing approaches, as at the lowest levels of
      analysis, elements will always remain opaque. Even in the most highly
      detailed test of logic possible, one that examines a RUT down to the
      individual logic gates, each gate would be treated as a black box.</p>

    <h3>Reference Output Based Testing</h3>

    <p>In <span class="term">reference output based testing</span>, an ordering
      is assigned to the <span class="term">inputs</span> for
      the routine under test, as well as to
      its <span class="term">outputs</span>. Through this ordering the inputs
      and outputs become vectors. Thus the routine under test is given
      an <span class="term">input vector</span> and it returns
      an <span class="term">observed output vector</span>.</p>

    <p>A <span class="term">Reference Model</span> is then 
      given the same input vector, and then it
      produces a <span class="term">reference output vector</span>. The reference
      output vector has the same component ordering as the 
      <span class="term">observed output vector</span>.

    <p>The <span class="term">failure detection function</span> then compares
    each observed output vector with its corresponding reference output vector. If
    they do not match, the test is deemed to have failed.</p>

    <p>It follows that in reference output based testing, the accuracy of the
    test results depends solely on the accuracy of the Reference Model.</p>

    <p>When the implementation of the Reference Model is unrelated to the
      routine under test, we tend to expect that the errors produced by the
      Reference Model will be uncorrelated with those produced by the routine
      under test, and thus not probable to coincide. This property will bias
      test routines towards delivering false fails. As noted earlier, false fails are
      likely to be caught as test fails are followed up with further
      scrutiny. It follows that reference output based testing can potentially
      deliver a high degree of accuracy even though the reference model is not
      ideal.</p>

    <h3>Property Check Testing</h3>

    <p><span class="term">Property Check Testing</span> is an alternative to
    reference output based testing. Here, rather than comparing each observed
    output to a reference output, the observed output is validated against
      known properties or expected characteristics.</p>

    <p>For example, given an integer as input, a function that correctly squares
      this input will preserve the parity of the input, as an odd number squared
      will be odd, and an even number squared will be even. The failure decider
      can check this property for each test case, and if it does not hold, the
      test case fails.</p>

    <p>Note for the square RUT test, this proposed property check is weak. Given
      a uniform distribution, half the time an errant square will still have the
      correct parity. There are stronger property checks that could be done for
      squares, but the point here is one of illustration.  A weak property check
      would not recognize many failures, and thus be biased towards false pass
      decisions. Those are the bad ones, as passing tests typically receive no
      further scrutiny.</p>

    <h3>Spot Checking</h3>

    <p>In spot checking, the function under test is checked against one or
      two input vectors.</p>

    <p>Moving from zero to one is an finite relative change, i.e., running a
      program for the first time requires that many moving parts work together,
      parts that have never been tried before; hence, a tremendous amount is
      learned about the logic and setup when the first test runs. Such a first
      test is called a <scan class="term">smoke test</scan>, a term that
      has literal meaning in the field of electronics testing.</p>

    <p>There are notorious edge cases in software. Zeros and index values just
      off the end of arrays come to mind. Checking a middle value and edge cases
      is often an effective approach for finding failures.</p>

    <p>It takes two points to determine a line. In Fourier analysis, it takes
      two samples per period of the highest frequency component to determine an
      entire waveform. Code also has patterns, patterns that are disjoint at
      edge cases. Hence if a piece of code runs without failures for both edge
      cases and spot check values in between, it will often run without
      failures over an entire domain of values. This effect explains why ad hoc
      testing has lead to so much relatively fail free code.</p>

    <p>Spot checking is especially valuable in early development, as it provides
      useful insights with minimal investment. At this stage, investing more is
      unwise while the code is still in flux.</p>

    <h3>Exhaustive Testing</h3>

    <p>A test routine will potentially run multiple test cases against a given
      RUT. If the RUT is a pure function, then per test case, a single test
      vector will be given to the RUT, and a single output vector will be
      returned.  However, if the RUT is sequential in nature, for each test case
      there will be a sequence of input vectors, and potentially a sequence of
      output vectors.</p>

    <p>The set of possible inputs for a RUT, were members are either individual
      vectors, or vector sequences, constitutes the <span class="term">input
      space</span>. Test <span class="term">coverage</span> is typically given
      as the proportion or inputs tested to the total in the input space,
      reported as a percentage./p>

    <p>When the RUT is a pure function, the input space is an enumeration of all
      possible input vectors. If the inputs include arbitrary long strings, then it
      will not be possible to complete such an enumeration, the best that can
      be done is to generate more and more inputs upon demand.
    </p>

     <p>When the RUT has sequential behavior, achieving full coverage requires
      giving the RUT every possible starting input, and then sequencing it to a
      point of hitting a stop state or cycle state in every possible way.  Again
      if inputs can be arbitrarily long strings, such an enumeration can not be
      completed. Furthermore, if the RUT state is encapsulated unseen in a black
      box, it might be very difficult, or impossible, to detect when the state
      has cycled.</p>

    <p><span class="term">Exhaustive testing</span> is said to have been
      done when every single input in the input space has been tested.
      An exhaustive test will have obtained 100% coverage, with no rounding
      done in the coverage computation.</p>

    <p>Suppose that a fault appears at time t₀. Suppose there is a duration of
      time of interest, Δ, that begins at or later than t₀. Suppose further
      there exists a given test and test case that fails due to the fault, but
      would not otherwise fail. Then a <span class="term">failure is
      reproducible</span> during Δ, if and only if the given test and test case
      would fail if run at any time during Δ, and no matter how many times it is
      run.</p>

    <p>For a RUT that is a pure function, this definition is the same as saying
      the test case fails at the same input value every time during Δ, when
      ideally is should have passed. For a sequential RUT, it is saying that the
      same input vector sequence will always lead to a failure, when ideally it
      would lead to a pass.</p>

    <p>Although the same test routine is run with identical inputs, a failure
      might not be reproducible due to other sources of variability, as
      examples:</p>
          <ol>
            <li>The contract made with the programmer for using the exact same
            inputs for the exact same test routine was broken.
            <li>Use of uninitialized memory.
            <li>Software updates or platform changes in between test runs during Δ.
            <li>Green thread, or real thread, scheduling differences, whether done by the OS or by the interpreter.
            <li>Using the system time as data, or other system parameter.
            <li>Race conditions.
            <li>Getting values from a randomly seeded pseudo random number generator.</li>
            <li>Reaching out of the architecture model for values, as examples
              using performance measures or by timing events.</li>
            <li>A hardware fault that is sensitive to a myriad of possible environmental
              influences.</li>
          </ol>
    
    <p>Exhaustive testing will find all failures that are reproducible. It might
      find failures that are not reproducible. The probability of witnessing
      non-reproducible failures will typically go up when using the technique
      of <span class="term">over testing</span>, i.e. running even more than an
      exhaustive number of tests.</p>

    <h2>Structured Testing</h2>

    <h3>The need for structured testing</h3>

    <p>All types of black box testing have a serious problem in that the search
      space for failures becomes exponentially larger as the number of inputs
      grow. Consider the case of the simplest of programs, one that adds two
      numbers together. When the RUT is a black box, the test routine only has
      access to the interface, so it appears like this.</p>

    <code>
      int8 sum(int8 a ,int8 b){
        ...
      }
    </code>
    
    


  </div>
</body>
</html>
  
<!--
discipline, if it was a bug, it should be test



    <p>A developer will use routines as building blocks for building
      a program. This leads to a hierarchy of routines.



    <p>A test of a single RUT that corresponds to a single routine in a program is
      known as a <span class="term">block test</span>. When the RUT encompasses
      multiple functions, it is called an <span class="term">integration 
        test</span>.</p>

    <p>A common structured testing approach is to first validate individual functions, then
      test their communication and interactions, and, finally, assess the complete
      integration of functions across a system.</p>


    structured testing

    <p>An important testing technique is to first test functions, then
      to test the communication between them, and then as a last step
      to test the integration of the functions.</p>


    sequential


    <p>To transform a routine with state variables into a more testable pure
      function, the internal memory is replaced by additional inputs. These
      inputs then  supply the memory values for each test.
 The values to be written to the
    memory can then be made into additional outputs. Additionally, the
    sequencing logic must be arranged to <span class="term">single-step</span>
    the routine, meaning that each call to the routine under test results in
    exactly one update to memory.</p>




    <p>A routine can be transformed into a function 
      by replacing the memory with further inputs that
      provide the memory value, adding further outputs that signify writes
      to the memory, and organizing the sequencing logic such that the
      routine <span class="term">single steps</span>, i.e. one write
      update to the memory occurs per call to the routine under test.</p>

    <p>Haskell, for example, provides a language semantic that makes testing
      of stateful routines more convenient. Short of such language support
      the process of converting routines to functions can be error prone
      itself, and lead to testing of a function that does not necessarily
      correspond to what would happen when testing the routine.</p>

    <p>Many languages employ the term <span class="term">function</span>
      to stand for a language construct, where said construct is not
      a function according to the formal definition of the term, but
      rather are routines. This started with the FORTRAN language, which
      distinguished functions from other routines, because they could
      return a single value that could be used in an expression, while
      routines in the language only passed values through arguments.
      In this guide, we will use the term routine to describe program
      units that do not fit the formal definition of function.</p>



    <p>Because the test routine only has access to the rut through its interfaces,
      the rut is said to be a black box. However, this term is misleading, as
      all computer code is accessed through inputs and outputs.  


-->

<!--  LocalWords:  decider's
 -->
